name: Terraform CI/CD with tfsec and SonarQube

on:
 # Trigger workflow on pushes to main branch, but only if user folders change
  push:
    branches: [main]
    paths:
      - 'user*/**'  
  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:  # Manual trigger

env:
      AWS_REGION: us-east-2
      KICS_S3_BUCKET: kics-log-bucket
      SQ_S3_BUCKET: sq-log-bucket
      PROJECT_NAME: htown-sp-sonarqube
      LAMBDA_ARTIFACT_BUCKET: htown-dev-lambda-artifacts
      DEFAULT_ROLE_ARN: arn:aws:iam::211125625532:role/htown-sp-iam-etl-dev  # set this secret once
      DEFAULT_GLUE_ROLE_ARN: arn:aws:iam::211125625532:role/htown-sp-glue-role-dev
      GLUE_SCRIPT_BUCKET: htown-dev-glue-scripts


      
jobs:

  # ===============================
  # 1. Detect which user folders changed
  # ===============================
  detect-folders:
    runs-on: ubuntu-latest
    outputs:
      folders: ${{ steps.set-folders.outputs.folders }}
    steps:
      - uses: actions/checkout@v4
        with:
           fetch-depth: 0 # Full history for git diff

      - id: set-folders
        run: |
          echo "Detecting user folders with changes..."
          folders=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep '^user' | cut -d/ -f1 | sort -u | uniq | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "folders=$folders" >> $GITHUB_OUTPUT

  # ===============================
  # 2. Run CI pipeline for each changed folder
  # ===============================        
  run-ci:
    name: Run Terraform CI/CD
    needs: detect-folders
    runs-on: ubuntu-latest
    if: needs.detect-folders.outputs.folders != '[]'
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    outputs:
      folder: ${{ matrix.folder }}  
    steps:

      # --- Checkout repo
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # --- Setup Terraform CLI
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      # --- Configure AWS credentials
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}  # Make this env variable

      # ---------------------------------------------------------
      # Step 1: Detect if this folder has any Lambda-related code
      # ---------------------------------------------------------
      - name: Detect lambda content
        id: detect-lambda
        run: |
          cd ${{ matrix.folder }}
          if [ -d python ] || [ -d layers ]; then
            echo "has_lambda=true" >> $GITHUB_OUTPUT
          else
            echo "has_lambda=false" >> $GITHUB_OUTPUT
          fi


      # ---------------------------------------------------------
      # Step 2: Zip Lambda function code and/or layer code
      # ---------------------------------------------------------
      - name: Zip Lambda code and layers (reproducible)
        if: steps.detect-lambda.outputs.has_lambda == 'true'
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p build

          # Function to normalize file modification times for reproducibility
          normalize_dir() {
            local dir="$1"
            # Normalize mtimes of tracked files so zip timestamps are stable
            if git ls-files "$dir" >/dev/null 2>&1; then
              git ls-files "$dir" | xargs -I{} touch -t 200001010000 {}
            else
              # Fallback if directory isn’t tracked (rare)
              find "$dir" -type f -print0 | xargs -0 -I{} touch -t 200001010000 {}
            fi
          }
          
          # Function to zip contents of a directory in a stable, sorted way
          make_zip() {
            # make_zip <src_dir> <zip_out>
            local src="$1"
            local out="$2"
            # Build a sorted list of files, excluding noise
            mapfile -t files < <(cd "$src" && \
              find . -type f \
                ! -path "*/__pycache__/*" \
                ! -name "*.pyc" \
                ! -path "*/.DS_Store" \
                ! -path "*/.venv/*" \
              | LC_ALL=C sort)
            # Create the zip with stable attrs (-X) and deterministic file order
            rm -f "$out"
            (cd "$src" && zip -X -q "$OLDPWD/$out" "${files[@]}")
          }

          # Zip each Lambda function under python/
          if [ -d python ]; then
            for dir in python/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              normalize_dir "$dir"
              make_zip "$dir" "build/${name}.zip"
            done
          fi

          # Zip each Lambda layer under layers/
          if [ -d layers ]; then
            for dir in layers/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              normalize_dir "$dir"
              make_zip "$dir" "build/${name}_layer.zip"
            done
          fi

          echo "Built zips:"; ls -l build || true

      # ---------------------------------------------------------
      # Step 3: Upload artifacts to S3 and decide if TF should redeploy
      # ---------------------------------------------------------
      - name: Upload artifacts and write overrides (gated by content hash)
        if: steps.detect-lambda.outputs.has_lambda == 'true'
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out

          BUCKET="${{ env.LAMBDA_ARTIFACT_BUCKET }}"
          CHECKSUM_PREFIX="checksums"  # S3 folder to store per-function content hashes

          # Compute a stable content hash from source files under python/<func> (paths + file contents)
          content_hash() {
            local dir="$1"   # e.g., python/data_cleaner
            mapfile -t files < <(cd "$dir" && \
              find . -type f \
                ! -path "*/__pycache__/*" \
                ! -name "*.pyc" \
                ! -path "*/.DS_Store" \
                ! -path "*/.venv/*" \
              | LC_ALL=C sort)
            if [ "${#files[@]}" -eq 0 ]; then echo "EMPTY"; return; fi
            tmp=$(mktemp)
            (
              cd "$dir"
              for f in "${files[@]}"; do
                printf "%s\0" "$f"
                openssl dgst -sha256 -binary "$f" | openssl base64 -A
                printf "\n"
              done
            ) > "$tmp"
            openssl dgst -sha256 -binary "$tmp" | openssl base64 -A
            rm -f "$tmp"
          }

          # Upload any Lambda layers to S3 (no TF redeploy needed for layers)
          if compgen -G "build/*_layer.zip" > /dev/null; then
            for zip in build/*_layer.zip; do
              base="$(basename "$zip" .zip)"; name="${base%_layer}"
              aws s3 cp "$zip" "s3://${BUCKET}/layers/${name}.zip"
            done
          fi
          
          # Prepare overrides JSON for Terraform
          echo '{ "source_code_hashes": {' > tfvars_out/source_code.tfvars.json
          first=1

          # For each function zip, compare source content hash to prior run; only emit TF hash on change
          if compgen -G "build/*.zip" > /dev/null; then
            for zip in build/*.zip; do
              base="$(basename "$zip" .zip)"
              [[ "$base" == *_layer ]] && continue
              func="$base"
              src_dir="python/${func}"

              if [ ! -d "$src_dir" ]; then
                # Fallback: no source dir found — upload & emit once
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
                zip_hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64 -A)
                if [ $first -eq 1 ]; then first=0; else echo "," >> tfvars_out/source_code.tfvars.json; fi
                echo "  \"${func}\": { \"source_code_hash\": \"${zip_hash}\" }" >> tfvars_out/source_code.tfvars.json
                continue
              fi
              
              # Compare new source hash to old one stored in S3
              new_ch=$(content_hash "$src_dir") || new_ch=""
              old_ch=""
              if aws s3 cp "s3://${BUCKET}/${CHECKSUM_PREFIX}/${func}.sha256" /tmp/prev.sha256 2>/dev/null; then
                old_ch="$(cat /tmp/prev.sha256 || true)"
              fi

              if [ -n "$new_ch" ] && [ "$new_ch" = "$old_ch" ]; then
                echo "No source change for ${func}; uploading zip but NOT emitting TF hash."
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
              else
                echo "Source change for ${func}; uploading & emitting TF hash."
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
                zip_hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64 -A)
                if [ $first -eq 1 ]; then first=0; else echo "," >> tfvars_out/source_code.tfvars.json; fi
                echo "  \"${func}\": { \"source_code_hash\": \"${zip_hash}\" }" >> tfvars_out/source_code.tfvars.json
                # Save new content hash to S3 for next run
                printf "%s" "$new_ch" > /tmp/new.sha256
                aws s3 cp /tmp/new.sha256 "s3://${BUCKET}/${CHECKSUM_PREFIX}/${func}.sha256" >/dev/null
              fi
            done
          else
            echo "ERROR: no function zips found in build/" >&2
            exit 1
          fi

          echo "} }" >> tfvars_out/source_code.tfvars.json
          echo "=== overrides JSON ==="
          cat tfvars_out/source_code.tfvars.json

 
            - name: Build Glue autogen tfvars from repo (glue.yaml optional)
              if: matrix.folder == 'user_glue'
              run: |
                set -euo pipefail
                cd ${{ matrix.folder }}
                mkdir -p tfvars_out

                GLUE_SCRIPT_BUCKET="${GLUE_SCRIPT_BUCKET:-htown-dev-glue-scripts}"
                DEFAULT_GLUE_ROLE_ARN="${DEFAULT_GLUE_ROLE_ARN:-}"   # set via env/secret if you want a default

                # Defaults for Glue
                DEFAULT_GLUE_VERSION="4.0"
                DEFAULT_MAX_RETRIES=0
                DEFAULT_TIMEOUT=60
                DEFAULT_WORKER_TYPE="G.1X"
                DEFAULT_NUM_WORKERS=2

                # yq for parsing glue.yaml (same pattern as lambda)
                if ! command -v yq >/dev/null 2>&1; then
                  wget -q https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64 -O /usr/local/bin/yq || true
                  chmod +x /usr/local/bin/yq || true
                fi

                echo '{ "glue_jobs_autogen": {' > tfvars_out/glue.auto.tfvars.json
                first=1

                shopt -s nullglob
                for job_dir in scripts/*; do
                  [ -d "$job_dir" ] || continue
                  name="$(basename "$job_dir")"

                  script_file="$job_dir/job.py"
                  if [ ! -f "$script_file" ]; then
                    echo "WARNING: no job.py found in $job_dir, skipping"
                    continue
                  fi

                  # Upload script to S3
                  s3_key="glue/${name}/job.py"
                  aws s3 cp "$script_file" "s3://${GLUE_SCRIPT_BUCKET}/${s3_key}"

                  glue_yaml="$job_dir/glue.yaml"

                  role_arn=""
                  glue_version=""
                  max_retries=""
                  timeout=""
                  worker_type=""
                  number_of_workers=""
                  connections="[]"
                  default_args="{}"

                  if [ -f "$glue_yaml" ] && command -v yq >/dev/null 2>&1; then
                    role_arn="$(yq -r '.role_arn // ""' "$glue_yaml")"
                    glue_version="$(yq -r '.glue_version // ""' "$glue_yaml")"
                    max_retries="$(yq -r '.max_retries // ""' "$glue_yaml")"
                    timeout="$(yq -r '.timeout // ""' "$glue_yaml")"
                    worker_type="$(yq -r '.worker_type // ""' "$glue_yaml")"
                    number_of_workers="$(yq -r '.number_of_workers // ""' "$glue_yaml")"
                    connections="$(yq -o=json '.connections // []' "$glue_yaml")"
                    default_args="$(yq -o=json '.default_arguments // {}' "$glue_yaml")"
                  fi

                  # Apply defaults when missing
                  glue_version="${glue_version:-$DEFAULT_GLUE_VERSION}"
                  max_retries="${max_retries:-$DEFAULT_MAX_RETRIES}"
                  timeout="${timeout:-$DEFAULT_TIMEOUT}"
                  worker_type="${worker_type:-$DEFAULT_WORKER_TYPE}"
                  number_of_workers="${number_of_workers:-$DEFAULT_NUM_WORKERS}"

                  # Default role if none set
                  if [ -z "$role_arn" ] && [ -n "$DEFAULT_GLUE_ROLE_ARN" ]; then
                    role_arn="$DEFAULT_GLUE_ROLE_ARN"
                  fi

                  if [ -z "$role_arn" ]; then
                    echo "ERROR: Glue job '$name' has no role_arn and DEFAULT_GLUE_ROLE_ARN is not set."
                    exit 1
                  fi

                  script_location="s3://${GLUE_SCRIPT_BUCKET}/${s3_key}"

                  [ $first -eq 1 ] && first=0 || echo "," >> tfvars_out/glue.auto.tfvars.json
                  cat >> tfvars_out/glue.auto.tfvars.json <<EOF
                  "${name}": {
                    "script_location": "${script_location}",
                    "role_arn": "${role_arn}",
                    "glue_version": "${glue_version}",
                    "max_retries": ${max_retries},
                    "timeout": ${timeout},
                    "worker_type": "${worker_type}",
                    "number_of_workers": ${number_of_workers},
                    "default_arguments": ${default_args},
                    "connections": ${connections},
                    "security_configuration": null,
                    "tags": {}
                   }
             EOF
          done

          echo "} }" >> tfvars_out/glue.auto.tfvars.json

          echo "=== Glue autogen tfvars ==="
          cat tfvars_out/glue.auto.tfvars.json

      - name: Write empty Glue overrides (no Glue jobs in this folder)
        if: matrix.folder == 'user_glue'
        run: |
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out
          if [ ! -d scripts ] || [ -z "$(ls -A scripts 2>/dev/null)" ]; then
            echo '{ "glue_jobs_autogen": {} }' > tfvars_out/glue.auto.tfvars.json
          fi


      - name: Build autogen tfvars from repo folders (lambda.yaml optional)
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out

          # ---- Defaults (used only when lambda.yaml omits a value) ----
          DEFAULT_RUNTIME="python3.12"
          DEFAULT_HANDLER="app.handler"
          DEFAULT_TIMEOUT=60
          DEFAULT_MEMORY=256
          DEFAULT_ROLE_ARN="${DEFAULT_ROLE_ARN:-}"  # from job env above (secret-backed)

          # Fail fast if we need a role but none is provided anywhere
          if [ -z "${DEFAULT_ROLE_ARN}" ]; then
          echo "WARNING: DEFAULT_ROLE_ARN is empty. Lambdas without lambda.yaml.role_arn will fail at apply."
          fi

          echo '{ "lambda_functions_autogen": {' > tfvars_out/lambdas.auto.tfvars.json
          first=1

          # yq is optional: only used if lambda.yaml present
          if ! command -v yq >/dev/null 2>&1; then
          wget -q https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64 -O /usr/local/bin/yq || true
          chmod +x /usr/local/bin/yq || true
          fi

          shopt -s nullglob
          for fn_dir in python/*; do
          [ -d "$fn_dir" ] || continue
          name="$(basename "$fn_dir")"

          yaml="$fn_dir/lambda.yaml"
          if [ -f "$yaml" ] && command -v yq >/dev/null 2>&1; then
          role_arn="$(yq -r '.role_arn // ""' "$yaml")"
          handler="$(yq -r '.handler // ""' "$yaml")"
          runtime="$(yq -r '.runtime // ""' "$yaml")"
          timeout="$(yq -r '.timeout // ""' "$yaml")"
          memory="$(yq -r '.memory_size // ""' "$yaml")"
          layers="$(yq -o=json '.layers // []' "$yaml")"
          envjson="$(yq -o=json '.environment // {}' "$yaml")"
          else
          role_arn=""
          handler=""
          runtime=""
          timeout=""
          memory=""
          layers="[]"
          envjson="{}"
          fi

          # ---- Apply defaults when fields are empty ----
          handler="${handler:-$DEFAULT_HANDLER}"
          runtime="${runtime:-$DEFAULT_RUNTIME}"
          timeout="${timeout:-$DEFAULT_TIMEOUT}"
          memory="${memory:-$DEFAULT_MEMORY}"

          # <- THIS is the key: inject default role when missing
          if [ -z "$role_arn" ] && [ -n "$DEFAULT_ROLE_ARN" ]; then
           role_arn="$DEFAULT_ROLE_ARN"
          fi

          # Optional guard: fail if still empty (prevents AWS 400)
          if [ -z "$role_arn" ]; then
           echo "ERROR: Lambda '$name' has no role_arn and DEFAULT_ROLE_ARN is not set."
           exit 1
          fi

          fn_s3_key="lambdas/${name}.zip"
          layer_s3_key="layers/${name}.zip"

          [ $first -eq 1 ] && first=0 || echo "," >> tfvars_out/lambdas.auto.tfvars.json
          cat >> tfvars_out/lambdas.auto.tfvars.json <<EOF
          "${name}": {
          "purpose": "${name}",
          "handler": "${handler}",
          "runtime": "${runtime}",
          "role_arn": "${role_arn}",
          "environment_variables": ${envjson},
          "s3_key": "${fn_s3_key}",
          "create_layer": false,
          "layer_s3_key": "${layer_s3_key}",
          "layers": ${layers},
          "timeout": ${timeout},
          "memory_size": ${memory}
          }
          EOF
          done
          echo "} }" >> tfvars_out/lambdas.auto.tfvars.json

          echo "=== autogen tfvars ==="
          cat tfvars_out/lambdas.auto.tfvars.json

      

      # ---------------------------------------------------------
      # Step 4: If no Lambda code exists, make an empty overrides file
      # ---------------------------------------------------------
      - name: Write empty overrides (no Lambda in this folder)
        if: steps.detect-lambda.outputs.has_lambda != 'true'
        run: |
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out
          echo '{ "source_code_hashes": {} }' > tfvars_out/source_code.tfvars.json
          echo '{ "lambda_functions_autogen": {} }' > tfvars_out/lambdas.auto.tfvars.json

      
      # ===============================
      # Terraform Plan Stage
      # ===============================
      - name: Upload tfvars_out to Artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out

      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}

      - name: Terraform Validate
        run: terraform validate
        working-directory: ${{ matrix.folder }}

      - name: Terraform Plan
        run: |
          if [ "${{ matrix.folder }}" = "user_glue" ]; then
            terraform plan -var-file="vars/dev.tfvars" -var-file="tfvars_out/glue.auto.tfvars.json"
          else
            terraform plan -var-file="vars/dev.tfvars" -var-file="tfvars_out/lambdas.auto.tfvars.json" -var-file="tfvars_out/source_code.tfvars.json"
          fi
        working-directory: ${{ matrix.folder }}


  # ===============================
  # Terraform Apply (manual approval)
  # ===============================
  terraform-apply:
    name: Terraform Apply
    needs: [run-ci, detect-folders]
    runs-on: ubuntu-latest
    environment:
      name: dev-apply  # Manual approval in GitHub Environments
      url: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download tfvars_out from Artifact
        uses: actions/download-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out
      
      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}

      - name: Terraform Apply
        run: |
          if [ "${{ matrix.folder }}" = "user_glue" ]; then
            terraform apply -auto-approve -var-file="vars/dev.tfvars" -var-file="tfvars_out/glue.auto.tfvars.json"
          else
            terraform apply -auto-approve -var-file="vars/dev.tfvars" -var-file="tfvars_out/lambdas.auto.tfvars.json" -var-file="tfvars_out/source_code.tfvars.json"
          fi
        working-directory: ${{ matrix.folder }}


        

      
