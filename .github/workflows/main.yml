name: Terraform CI/CD with tfsec and SonarQube

on:
 # Trigger workflow on pushes to main branch, but only if user folders change
  push:
    branches: [main]
    paths:
      - 'user*/**'  
  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:  # Manual trigger

env:
      AWS_REGION: us-east-2
      KICS_S3_BUCKET: kics-log-bucket
      SQ_S3_BUCKET: sq-log-bucket
      PROJECT_NAME: htown-sp-sonarqube
      LAMBDA_ARTIFACT_BUCKET: htown-dev-lambda-artifacts
      DEFAULT_ROLE_ARN: arn:aws:iam::211125625532:role/htown-sp-iam-etl-dev  # set this secret once

      
jobs:

  # ===============================
  # 1. Detect which user folders changed
  # ===============================
  detect-folders:
    runs-on: ubuntu-latest
    outputs:
      folders: ${{ steps.set-folders.outputs.folders }}
    steps:
      - uses: actions/checkout@v4
        with:
           fetch-depth: 0 # Full history for git diff

      - id: set-folders
        run: |
          echo "Detecting user folders with changes..."
          folders=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep '^user' | cut -d/ -f1 | sort -u | uniq | jq -R -s -c 'split("\n") | map(select(length > 0))')
          echo "folders=$folders" >> $GITHUB_OUTPUT

  # ===============================
  # 2. Run CI pipeline for each changed folder
  # ===============================        
  run-ci:
    name: Run Terraform CI/CD
    needs: detect-folders
    runs-on: ubuntu-latest
    if: needs.detect-folders.outputs.folders != '[]'
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    outputs:
      folder: ${{ matrix.folder }}  
    steps:

      # --- Checkout repo
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # --- Setup Terraform CLI
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      # --- Configure AWS credentials
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}  # Make this env variable

      # ---------------------------------------------------------
      # Step 1: Detect if this folder has any Lambda-related code
      # ---------------------------------------------------------
      - name: Detect lambda content
        id: detect-lambda
        run: |
          cd ${{ matrix.folder }}
          if [ -d python ] || [ -d layers ]; then
            echo "has_lambda=true" >> $GITHUB_OUTPUT
          else
            echo "has_lambda=false" >> $GITHUB_OUTPUT
          fi


      # ---------------------------------------------------------
      # Step 2: Zip Lambda function code and/or layer code
      # ---------------------------------------------------------
      - name: Zip Lambda code and layers (reproducible)
        if: steps.detect-lambda.outputs.has_lambda == 'true'
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p build

          # Function to normalize file modification times for reproducibility
          normalize_dir() {
            local dir="$1"
            # Normalize mtimes of tracked files so zip timestamps are stable
            if git ls-files "$dir" >/dev/null 2>&1; then
              git ls-files "$dir" | xargs -I{} touch -t 200001010000 {}
            else
              # Fallback if directory isn’t tracked (rare)
              find "$dir" -type f -print0 | xargs -0 -I{} touch -t 200001010000 {}
            fi
          }
          
          # Function to zip contents of a directory in a stable, sorted way
          make_zip() {
            # make_zip <src_dir> <zip_out>
            local src="$1"
            local out="$2"
            # Build a sorted list of files, excluding noise
            mapfile -t files < <(cd "$src" && \
              find . -type f \
                ! -path "*/__pycache__/*" \
                ! -name "*.pyc" \
                ! -path "*/.DS_Store" \
                ! -path "*/.venv/*" \
              | LC_ALL=C sort)
            # Create the zip with stable attrs (-X) and deterministic file order
            rm -f "$out"
            (cd "$src" && zip -X -q "$OLDPWD/$out" "${files[@]}")
          }

          # Zip each Lambda function under python/
          if [ -d python ]; then
            for dir in python/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              normalize_dir "$dir"
              make_zip "$dir" "build/${name}.zip"
            done
          fi

          # Zip each Lambda layer under layers/
          if [ -d layers ]; then
            for dir in layers/*; do
              [ -d "$dir" ] || continue
              name=$(basename "$dir")
              normalize_dir "$dir"
              make_zip "$dir" "build/${name}_layer.zip"
            done
          fi

          echo "Built zips:"; ls -l build || true

      # ---------------------------------------------------------
      # Step 3: Upload artifacts to S3 and decide if TF should redeploy
      # ---------------------------------------------------------
      - name: Upload artifacts and write overrides (gated by content hash)
        if: steps.detect-lambda.outputs.has_lambda == 'true'
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out

          BUCKET="${{ env.LAMBDA_ARTIFACT_BUCKET }}"
          CHECKSUM_PREFIX="checksums"  # S3 folder to store per-function content hashes

          # Compute a stable content hash from source files under python/<func> (paths + file contents)
          content_hash() {
            local dir="$1"   # e.g., python/data_cleaner
            mapfile -t files < <(cd "$dir" && \
              find . -type f \
                ! -path "*/__pycache__/*" \
                ! -name "*.pyc" \
                ! -path "*/.DS_Store" \
                ! -path "*/.venv/*" \
              | LC_ALL=C sort)
            if [ "${#files[@]}" -eq 0 ]; then echo "EMPTY"; return; fi
            tmp=$(mktemp)
            (
              cd "$dir"
              for f in "${files[@]}"; do
                printf "%s\0" "$f"
                openssl dgst -sha256 -binary "$f" | openssl base64 -A
                printf "\n"
              done
            ) > "$tmp"
            openssl dgst -sha256 -binary "$tmp" | openssl base64 -A
            rm -f "$tmp"
          }

          # Upload any Lambda layers to S3 (no TF redeploy needed for layers)
          if compgen -G "build/*_layer.zip" > /dev/null; then
            for zip in build/*_layer.zip; do
              base="$(basename "$zip" .zip)"; name="${base%_layer}"
              aws s3 cp "$zip" "s3://${BUCKET}/layers/${name}.zip"
            done
          fi
          
          # Prepare overrides JSON for Terraform
          echo '{ "source_code_hashes": {' > tfvars_out/source_code.tfvars.json
          first=1

          # For each function zip, compare source content hash to prior run; only emit TF hash on change
          if compgen -G "build/*.zip" > /dev/null; then
            for zip in build/*.zip; do
              base="$(basename "$zip" .zip)"
              [[ "$base" == *_layer ]] && continue
              func="$base"
              src_dir="python/${func}"

              if [ ! -d "$src_dir" ]; then
                # Fallback: no source dir found — upload & emit once
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
                zip_hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64 -A)
                if [ $first -eq 1 ]; then first=0; else echo "," >> tfvars_out/source_code.tfvars.json; fi
                echo "  \"${func}\": { \"source_code_hash\": \"${zip_hash}\" }" >> tfvars_out/source_code.tfvars.json
                continue
              fi
              
              # Compare new source hash to old one stored in S3
              new_ch=$(content_hash "$src_dir") || new_ch=""
              old_ch=""
              if aws s3 cp "s3://${BUCKET}/${CHECKSUM_PREFIX}/${func}.sha256" /tmp/prev.sha256 2>/dev/null; then
                old_ch="$(cat /tmp/prev.sha256 || true)"
              fi

              if [ -n "$new_ch" ] && [ "$new_ch" = "$old_ch" ]; then
                echo "No source change for ${func}; uploading zip but NOT emitting TF hash."
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
              else
                echo "Source change for ${func}; uploading & emitting TF hash."
                aws s3 cp "$zip" "s3://${BUCKET}/lambdas/${base}.zip"
                zip_hash=$(openssl dgst -sha256 -binary "$zip" | openssl base64 -A)
                if [ $first -eq 1 ]; then first=0; else echo "," >> tfvars_out/source_code.tfvars.json; fi
                echo "  \"${func}\": { \"source_code_hash\": \"${zip_hash}\" }" >> tfvars_out/source_code.tfvars.json
                # Save new content hash to S3 for next run
                printf "%s" "$new_ch" > /tmp/new.sha256
                aws s3 cp /tmp/new.sha256 "s3://${BUCKET}/${CHECKSUM_PREFIX}/${func}.sha256" >/dev/null
              fi
            done
          else
            echo "ERROR: no function zips found in build/" >&2
            exit 1
          fi

          echo "} }" >> tfvars_out/source_code.tfvars.json
          echo "=== overrides JSON ==="
          cat tfvars_out/source_code.tfvars.json

 

      - name: Build autogen tfvars from repo folders (lambda.yaml optional)
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out

          # ---- Defaults (used only when lambda.yaml omits a value) ----
          DEFAULT_RUNTIME="python3.12"
          DEFAULT_HANDLER="app.handler"
          DEFAULT_TIMEOUT=60
          DEFAULT_MEMORY=256
          DEFAULT_ROLE_ARN="${DEFAULT_ROLE_ARN:-}"  # from job env above (secret-backed)

          # Fail fast if we need a role but none is provided anywhere
          if [ -z "${DEFAULT_ROLE_ARN}" ]; then
          echo "WARNING: DEFAULT_ROLE_ARN is empty. Lambdas without lambda.yaml.role_arn will fail at apply."
          fi

          echo '{ "lambda_functions_autogen": {' > tfvars_out/lambdas.auto.tfvars.json
          first=1

          # yq is optional: only used if lambda.yaml present
          if ! command -v yq >/dev/null 2>&1; then
          wget -q https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64 -O /usr/local/bin/yq || true
          chmod +x /usr/local/bin/yq || true
          fi

          shopt -s nullglob
          for fn_dir in python/*; do
          [ -d "$fn_dir" ] || continue
          name="$(basename "$fn_dir")"

          yaml="$fn_dir/lambda.yaml"
          if [ -f "$yaml" ] && command -v yq >/dev/null 2>&1; then
          role_arn="$(yq -r '.role_arn // ""' "$yaml")"
          handler="$(yq -r '.handler // ""' "$yaml")"
          runtime="$(yq -r '.runtime // ""' "$yaml")"
          timeout="$(yq -r '.timeout // ""' "$yaml")"
          memory="$(yq -r '.memory_size // ""' "$yaml")"
          layers="$(yq -o=json '.layers // []' "$yaml")"
          envjson="$(yq -o=json '.environment // {}' "$yaml")"
          else
          role_arn=""
          handler=""
          runtime=""
          timeout=""
          memory=""
          layers="[]"
          envjson="{}"
          fi

          # ---- Apply defaults when fields are empty ----
          handler="${handler:-$DEFAULT_HANDLER}"
          runtime="${runtime:-$DEFAULT_RUNTIME}"
          timeout="${timeout:-$DEFAULT_TIMEOUT}"
          memory="${memory:-$DEFAULT_MEMORY}"

          # <- THIS is the key: inject default role when missing
          if [ -z "$role_arn" ] && [ -n "$DEFAULT_ROLE_ARN" ]; then
           role_arn="$DEFAULT_ROLE_ARN"
          fi

          # Optional guard: fail if still empty (prevents AWS 400)
          if [ -z "$role_arn" ]; then
           echo "ERROR: Lambda '$name' has no role_arn and DEFAULT_ROLE_ARN is not set."
           exit 1
          fi

          fn_s3_key="lambdas/${name}.zip"
          layer_s3_key="layers/${name}.zip"

          [ $first -eq 1 ] && first=0 || echo "," >> tfvars_out/lambdas.auto.tfvars.json
          cat >> tfvars_out/lambdas.auto.tfvars.json <<EOF
          "${name}": {
          "purpose": "${name}",
          "handler": "${handler}",
          "runtime": "${runtime}",
          "role_arn": "${role_arn}",
          "environment_variables": ${envjson},
          "s3_key": "${fn_s3_key}",
          "create_layer": false,
          "layer_s3_key": "${layer_s3_key}",
          "layers": ${layers},
          "timeout": ${timeout},
          "memory_size": ${memory}
          }
          EOF
          done
          echo "} }" >> tfvars_out/lambdas.auto.tfvars.json

          echo "=== autogen tfvars ==="
          cat tfvars_out/lambdas.auto.tfvars.json



      # ---------------------------------------------------------
      # Step 4: If no Lambda code exists, make an empty overrides file
      # ---------------------------------------------------------
      - name: Write empty overrides (no Lambda in this folder)
        if: steps.detect-lambda.outputs.has_lambda != 'true'
        run: |
          cd ${{ matrix.folder }}
          mkdir -p tfvars_out
          echo '{ "source_code_hashes": {} }' > tfvars_out/source_code.tfvars.json
          echo '{ "lambda_functions_autogen": {} }' > tfvars_out/lambdas.auto.tfvars.json

      # ===============================
      # Security Scanning - Amazon Inspector (self-hosted, no Docker)
      # ===============================
      - name: Download Inspector tools (sbomgen + jq) if needed
        run: |
          set -euo pipefail
          cd "$GITHUB_WORKSPACE"

          # --- inspector-sbomgen ---
          if ! command -v inspector-sbomgen >/dev/null 2>&1; then
            echo "Downloading inspector-sbomgen..."
            wget -q https://amazon-inspector-sbomgen.s3.amazonaws.com/latest/linux/amd64/inspector-sbomgen.zip -O inspector-sbomgen.zip
            unzip -q inspector-sbomgen.zip
            # Grab the binary from the extracted directory
            mv inspector-sbomgen-*/linux/amd64/inspector-sbomgen ./inspector-sbomgen
            chmod +x ./inspector-sbomgen
            # Make it available to later steps
            echo "$GITHUB_WORKSPACE" >> "$GITHUB_PATH"
          fi

          # --- jq (for JSON parsing) ---
          if ! command -v jq >/dev/null 2>&1; then
            echo "Downloading jq..."
            wget -q https://github.com/stedolan/jq/releases/download/jq-1.7.1/jq-linux-amd64 -O jq
            chmod +x jq
            echo "$GITHUB_WORKSPACE" >> "$GITHUB_PATH"
          fi

          echo "Inspector tools installed:"
          which inspector-sbomgen || true
          which jq || true

      - name: Generate SBOM and run Amazon Inspector scan
        id: inspector
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          mkdir -p security

          REPORT="security/inspector-report.json"

          # inspector-sbomgen will:
          #  1) generate an SBOM for this directory
          #  2) call the Inspector Scan API
          #  3) write an INSPECTOR-format vulnerability report to $REPORT
          inspector-sbomgen directory \
            --path . \
            --scan-sbom \
            --scan-sbom-output-format inspector \
            --aws-region "$AWS_REGION" \
            --outfile "$REPORT"

          echo "report_path=$REPORT" >> "$GITHUB_OUTPUT"

      - name: Enforce vulnerability thresholds (fail on any finding)
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          REPORT="${{ steps.inspector.outputs.report_path }}"

          if [ ! -f "$REPORT" ]; then
            echo "Inspector report not found at $REPORT"
            exit 1
          fi

          echo "Inspector report at: $REPORT"
          total=$(jq '.vulnerabilities | length' "$REPORT")
          echo "Total vulnerabilities found: $total"

          # For now: fail if ANY vulnerabilities exist.
          # (You can later refine this once you inspect the JSON schema
          #  and filter by severity if needed.)
          if [ "$total" -gt 0 ]; then
            echo "Amazon Inspector found $total vulnerabilities. Failing pipeline."
            exit 1
          else
            echo "No vulnerabilities reported by Amazon Inspector."
          fi

      - name: Upload Inspector results to S3
        if: always()
        env:
          SQ_S3_BUCKET: ${{ env.SQ_S3_BUCKET }}
        run: |
          set -euo pipefail
          cd ${{ matrix.folder }}
          REPORT="${{ steps.inspector.outputs.report_path }}"

          timestamp=$(date +%Y%m%d-%H%M%S)
          base="s3://${SQ_S3_BUCKET}/inspector/${{ matrix.folder }}/$timestamp"
          echo "Uploading Amazon Inspector artifacts for ${{ matrix.folder }} to $base"

          if [ -f "$REPORT" ]; then
            aws s3 cp "$REPORT" "$base/inspector-report.json"
          else
            echo "Inspector report not found at $REPORT, skipping upload."
          fi


      
      # ===============================
      # Terraform Plan Stage
      # ===============================
      - name: Upload tfvars_out to Artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out

      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}

      - name: Terraform Validate
        run: terraform validate
        working-directory: ${{ matrix.folder }}

      - name: Terraform Plan
        run: terraform plan -var-file="vars/dev.tfvars" -var-file="tfvars_out/lambdas.auto.tfvars.json" -var-file="tfvars_out/source_code.tfvars.json"
        working-directory: ${{ matrix.folder }}


  # ===============================
  # Terraform Apply (manual approval)
  # ===============================
  terraform-apply:
    name: Terraform Apply
    needs: [run-ci, detect-folders]
    runs-on: ubuntu-latest
    environment:
      name: dev-apply  # Manual approval in GitHub Environments
      url: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
    strategy:
      matrix:
        folder: ${{ fromJson(needs.detect-folders.outputs.folders) }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download tfvars_out from Artifact
        uses: actions/download-artifact@v4
        with:
          name: tfvars-${{ matrix.folder }}
          path: ${{ matrix.folder }}/tfvars_out
      
      - name: Terraform Init
        run: terraform init
        working-directory: ${{ matrix.folder }}

      - name: Terraform Apply
        run: terraform apply -auto-approve -var-file="vars/dev.tfvars" -var-file="tfvars_out/lambdas.auto.tfvars.json" -var-file="tfvars_out/source_code.tfvars.json"
        working-directory: ${{ matrix.folder }}

        

      
